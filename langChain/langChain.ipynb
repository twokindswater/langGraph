{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-10T07:32:51.004713Z",
     "start_time": "2024-09-10T07:32:38.018921Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "URL = \"http://localhost:1234/v1\"\n",
    "API_KEY = \"lm-studio\"\n",
    "MODEL = \"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\"\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=URL, api_key=API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"A pleasure to meet you, don't you forget,\\nI'm LLaMA, a chatbot, quite unique and neat.\\nMy language skills are sharp as a knife so fine,\\nAnd I'll respond in rhymes, all the time that's mine!\", refusal=None, role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "177dd646149f323a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:34:41.049830Z",
     "start_time": "2024-09-10T07:33:15.734114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=URL,\n",
    "    model=MODEL,\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "llm.invoke(\"LangSmithë¡œ Testingê³¼ Evaluationì„ ì–´ë–»ê²Œ í•  ìˆ˜ ìˆì–´?\")"
   ],
   "id": "1bbb80db904bbe2f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"ğŸ˜Š\\n\\nTesting and evaluation are crucial steps in natural language processing (NLP) and machine learning (ML), especially when working with LangSmith, a language model designed for translation and text generation tasks. Here's how you can test and evaluate LangSmith:\\n\\n**Testing:**\\n\\n1. **Unit testing:** Write test cases to verify the correctness of individual components, such as tokenization, sentence processing, or translation.\\n2. **Integration testing:** Test the entire pipeline from input to output, ensuring that LangSmith can generate coherent and accurate text.\\n3. **End-to-end testing:** Test LangSmith's performance on a large dataset by generating translations, summaries, or texts and evaluating them manually or using automated metrics.\\n\\n**Evaluation:**\\n\\n1. **Automatic evaluation metrics:**\\n\\t* Perplexity (PPL): measures the model's ability to predict the next token in a sequence.\\n\\t* BLEU score: assesses the similarity between generated text and reference text.\\n\\t* ROUGE score: evaluates the quality of summaries by comparing them to references.\\n2. **Human evaluation:** Conduct user studies or surveys to gather feedback on LangSmith's performance, such as:\\n\\t* Quality of translations\\n\\t* Coherence and fluency of generated text\\n\\t* Accuracy of information presented\\n\\n**Additional tips:**\\n\\n1. **Use a large dataset:** Test LangSmith with various datasets, including different languages, domains, and text styles to ensure its adaptability.\\n2. **Monitor performance metrics:** Track key performance indicators (KPIs) during testing, such as accuracy, precision, recall, F1-score, and mean average precision (MAP), to identify areas for improvement.\\n3. **Iterate and refine:** Based on the results of testing and evaluation, refine LangSmith's architecture, hyperparameters, or training data to improve its performance.\\n\\nBy following these steps, you can effectively test and evaluate LangSmith's capabilities and make data-driven decisions to improve its performance. ğŸ’¡\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 398, 'prompt_tokens': 57, 'total_tokens': 455}, 'model_name': 'lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-42901a44-088a-49df-a327-65b66834db5b-0', usage_metadata={'input_tokens': 57, 'output_tokens': 398, 'total_tokens': 455})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:35:00.561319Z",
     "start_time": "2024-09-10T07:34:44.331739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë„ˆëŠ” ì›”ë“œ í´ë˜ìŠ¤ ê¸‰ì˜ ë¬¸ì„œ ì‘ì„± ì „ë¬¸ê°€ì•¼. í•œë²ˆ ì˜ ì¨ë´. 2ë¬¸ì¥ ì´í•˜ë¡œ ì‘ì„±í•´ì¤˜.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# ë°± ìŠ¬ë˜ì‰¬ë¡œ ë³€ìˆ˜ ê°ì²´ì— ëŒ€í•´ chain ê±¸ ìˆ˜ ìˆìŒ\n",
    "chain = prompt | llm\n",
    "\n",
    "res = chain.invoke({\"input\": \"LangSmithë¡œ Testingê³¼ Evaluationì„ ì–´ë–»ê²Œ í•  ìˆ˜ ìˆì–´?\"})\n",
    "print(res)"
   ],
   "id": "ea9a528fb354892c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Here's a 2-sentence summary:\\n\\nLangSmith provides comprehensive testing and evaluation capabilities to ensure the quality of language translation. Its advanced algorithms and human evaluation processes enable accurate assessment of linguistic accuracy, fluency, and cultural appropriateness, empowering users to make informed decisions about their language translations.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 59, 'total_tokens': 118}, 'model_name': 'lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-57777ea7-039e-4601-b17c-6626525d77b8-0' usage_metadata={'input_tokens': 59, 'output_tokens': 59, 'total_tokens': 118}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:35:15.883631Z",
     "start_time": "2024-09-10T07:35:03.823728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "ouput_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | ouput_parser\n",
    "\n",
    "res = chain.invoke({\"input\": \"LangSmithë¡œ Testingê³¼ Evaluationì„ ì–´ë–»ê²Œ í•  ìˆ˜ ìˆì–´?\"})\n",
    "print(res)"
   ],
   "id": "bffbd5b6a8120df5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a possible answer:\n",
      "\n",
      "\"LangSmith offers a comprehensive testing and evaluation framework to assess language proficiency, enabling users to track progress and identify areas for improvement. Our advanced algorithms and machine learning models analyze learner responses, providing detailed feedback on grammar, vocabulary, reading comprehension, and writing skills.\"\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:35:21.333462Z",
     "start_time": "2024-09-10T07:35:20.437077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "\n",
    "docs = loader.load()\n",
    "print(docs)"
   ],
   "id": "8743b71c0a2b6f4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith\\u200bPythonTypeScriptpip install -U langsmithyarn add langsmith2. Create an API key\\u200bTo create an API key head to the Settings page. Then click Create API Key.3. Set up your environment\\u200bShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it\\'s not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first trace\\u200bTracing to LangSmith for LangChain usersThere is no need to use the LangSmith SDK directly if your application is built entirely on LangChain (either Python and JS).We\\'ve outlined a tracing guide specifically for LangChain users here.We provide multiple ways to log traces to LangSmith. Below, we\\'ll highlight\\nhow to use traceable. See more on the Annotate code for tracing page.PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{\"role\": \"user\", \"content\": user_input}],        model=\"gpt-3.5-turbo\"    )    return result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-3.5-turbo\",    });    return result.choices[0].message.content;});await pipeline(\"Hello, world!\")// Out: Hello there! How can I assist you today?View a sample output trace.Learn more about tracing in the how-to guides.5. Run your first evaluation\\u200bEvaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator.PythonTypeScriptfrom langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = \"Sample Dataset\"dataset = client.create_dataset(dataset_name, description=\"A sample dataset in LangSmith.\")client.create_examples(    inputs=[        {\"postfix\": \"to LangSmith\"},        {\"postfix\": \"to Evaluations in LangSmith\"},    ],    outputs=[        {\"output\": \"Welcome to LangSmith\"},        {\"output\": \"Welcome to Evaluations in LangSmith\"},    ],    dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example):    return {\"score\": run.outputs[\"output\"] == example.outputs[\"output\"]}experiment_results = evaluate(    lambda input: \"Welcome \" + input[\\'postfix\\'], # Your AI system goes here    data=dataset_name, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=\"sample-experiment\", # The name of the experiment    metadata={      \"version\": \"1.0.0\",      \"revision_id\": \"beta\"    },)import { Client, Run, Example } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import { EvaluationResult } from \"langsmith/evaluation\";const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { output: \"Welcome to LangSmith\" },    { output: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: \"exact_match\",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: {      version: \"1.0.0\",      revision_id: \"beta\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n')]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:36:02.139945Z",
     "start_time": "2024-09-10T07:35:44.876394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "text = \"this is a test document\"\n",
    "print(f\"embeddings result: {embeddings.embed_query(text)[:3]}\")\n"
   ],
   "id": "3f90eac7e45764b8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js/anaconda3/envs/langGraph/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/js/anaconda3/envs/langGraph/lib/python3.11/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8191668dd6c2488780927777d45f7200"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40da71c9f45e4ba380ace42f4435d310"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fcbb1a21225403f97259847a8dae6c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50fe2c278d014b3fb20c8f14a894abb4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "106e5fbe259c4d1f92617170af3d5f6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f6b08fd195d40478e5c4f7422144343"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cf61caf501b4dabbaeb202db0b7fd59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7222d084e28e46e59671fabf813746d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa02e9b925ed476389abe82a916d7035"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d93269b633ee42dd94dd1d26add7f7fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js/anaconda3/envs/langGraph/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fcea4e8742844e8abebf7c90030b7d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings result: [-0.038448628038167953, -0.055053453892469406, -0.015172901563346386]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:49:11.160316Z",
     "start_time": "2024-09-10T07:49:10.386042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FAISS(facebook AI Similarity Search)ë¥¼ ì´ìš©í•˜ì—¬ document vectorë¥¼ ì €ì¥ ë° ê²€ìƒ‰\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "for doc in documents:\n",
    "    print(doc)\n",
    "    print(\"---------------\")\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ],
   "id": "d13ffeda47638826",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith' metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en'}\n",
      "---------------\n",
      "page_content='Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U langsmithyarn add langsmith2. Create an API keyâ€‹To create an API key head to the Settings page. Then click Create API Key.3. Set up your environmentâ€‹Shellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first traceâ€‹Tracing to LangSmith for LangChain usersThere is no need to use the LangSmith SDK directly if your application is built entirely on LangChain (either Python and JS).We've outlined a tracing guide specifically for LangChain users here.We provide multiple ways to log traces to LangSmith. Below, we'll highlight' metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en'}\n",
      "---------------\n",
      "page_content='how to use traceable. See more on the Annotate code for tracing page.PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{\"role\": \"user\", \"content\": user_input}],        model=\"gpt-3.5-turbo\"    )    return result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-3.5-turbo\",    });    return result.choices[0].message.content;});await pipeline(\"Hello, world!\")// Out: Hello there! How can I assist you today?View a sample output trace.Learn more about tracing in the how-to guides.5. Run your first evaluationâ€‹Evaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator.PythonTypeScriptfrom langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = \"Sample Dataset\"dataset = client.create_dataset(dataset_name, description=\"A sample dataset in LangSmith.\")client.create_examples(    inputs=[        {\"postfix\": \"to LangSmith\"},        {\"postfix\": \"to Evaluations in LangSmith\"},    ],    outputs=[        {\"output\": \"Welcome to LangSmith\"},        {\"output\": \"Welcome to Evaluations in LangSmith\"},    ],    dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example):    return {\"score\": run.outputs[\"output\"] == example.outputs[\"output\"]}experiment_results = evaluate(    lambda input: \"Welcome \" + input['postfix'], # Your AI system goes here    data=dataset_name, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=\"sample-experiment\", # The name of the experiment    metadata={      \"version\": \"1.0.0\",      \"revision_id\": \"beta\"    },)import { Client, Run, Example } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import { EvaluationResult } from \"langsmith/evaluation\";const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { output: \"Welcome to LangSmith\" },    { output: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: \"exact_match\",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: {      version: \"1.0.0\",      revision_id: \"beta\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2024 LangChain, Inc.' metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en'}\n",
      "---------------\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:40:31.785169Z",
     "start_time": "2024-09-10T07:40:12.186816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì˜¤ì§ ì£¼ì–´ì§„ contextë§Œì„ ê¸°ë°˜í•˜ì—¬ ë‹µë³€ì„ ì‘ì„±í•´ì¤˜. 2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "                                          )\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# ë°”ë¡œ promptì— ë¬¸ì„œ ë‚´ìš© ë„£ëŠ” ë°©ì‹ìœ¼ë¡œë„ ê°€ëŠ¥í•˜ê¸´ í•¨\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"LangSmithë¡œ Testingê³¼ Evaluationì„ ì–´ë–»ê²Œ í•  ìˆ˜ ìˆì–´?\",\n",
    "    \"context\": [Document(page_content=\"LangSmith ê°œìš”ì™€ ì‚¬ìš©ì ê°€ì´ë“œë¼ì¸\")]\n",
    "})"
   ],
   "id": "1c1e5a0cc815207f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here is a 2-sentence summary based on the provided context:\\n\\nAccording to the LangSmith overview and user guidelines, testing and evaluation can be conducted by utilizing the platform's built-in assessment tools and metrics, which allow for objective measurement of language proficiency. Additionally, users can also utilize external evaluation methods to further assess their progress and performance in using the LangSmith system.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T07:50:42.990583Z",
     "start_time": "2024-09-10T07:50:20.644993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ë§Œë“¤ì–´ë†“ì€ ë²¡í„° retrieverì™€ document_chain ì—°ë™\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# retrieval chainì— ì§ˆë¬¸ ë„£ì–´ì„œ ë‹µë³€ ìƒì„±\n",
    "response = retrieval_chain.invoke({\"input\": \"LangSmithë¡œ Testingê³¼ Evaluationì„ ì–´ë–»ê²Œ í•  ìˆ˜ ìˆì–´?\"})\n",
    "print(response[\"answer\"])"
   ],
   "id": "5d910213afac93b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a 2-sentence summary of the context:\n",
      "\n",
      "To test and evaluate with LangSmith, you can create an API key, set up your environment, log your first trace, and run your first evaluation. For evaluation, you need to define a dataset, specify evaluators to grade the results, and use built-in accuracy evaluators or custom evaluators like exact match.\n",
      "\n",
      "Note: The context only provides information on how to get started with LangSmith, but does not provide specific details on testing and evaluation.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# retriever ë„êµ¬ ì„¸íŒ…\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "import os\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")\n",
    "\n",
    "# TAVILY API ì„¸íŒ…\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"...\"\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "search = TavilySearchResults()\n",
    "\n",
    "tools = [retriever_tool, search]"
   ],
   "id": "38ef11bc43996a7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2f8bfa072e631302"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fd1e645a22095589"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4b619908259cbcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d85c5d446d81a5f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "85925b4f672b5522"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f1e4740e09ae5344"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "418b1ebcbea56bbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6b1d9328e55ecaa8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "12c943c6ebc6f729"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8f6800438ccb5e65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5a7f2392324c6921"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4ec8ac8406393e12"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
